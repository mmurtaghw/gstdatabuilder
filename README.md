# Web Scraping Wikipedia and DBPedia, Data Cleaning, and RDF Testing with Python

This repository contains three Python scripts that facilitate the process of scraping Wikipedia pages, fetching corresponding DBPedia data, cleaning the resulting dataset, and testing the creation of RDF triples from the gathered data.

## Dependencies

The scripts in this repository depend on several Python packages:

* `requests` for making HTTP requests.
* `beautifulsoup4` for parsing HTML.
* `SPARQLWrapper` for querying DBPedia.
* `pandas` for managing and saving data.
* `tqdm` for showing a progress bar.
* `lxml` as a parser for BeautifulSoup.
* `rdflib` for creating and working with RDF triples.

All requirements can be installed using pip:

```
pip install -r requirements.txt
```

## Scripts

### 1. wikipedia_dbpedia_scraper.py

This script automates the process of scraping random Wikipedia articles, parsing the information, and storing it into a CSV file. It fetches the title and content of each Wikipedia page and attempts to fetch corresponding structured information from DBPedia.

### 2. testRDF.py

This script reads the CSV file generated by the previous script, extracts DBPedia data from the `dbpediaTriples` column, and creates an RDF graph using the `rdflib` library. It also prints out the total number of triples in the graph.

### 3. dataInfoAndCleaning.py

This script reads the CSV file, provides a summary of the data, checks for missing values, removes any rows with missing values, checks for duplicate entries, removes any duplicate entries, and writes the cleaned data back into the CSV file.

## Usage

The scripts can be run with Python 3.7 and later versions:

```
python getData.py
python testRDF.py
python dataInfoAndCleaning.py
```

## Output

The scripts write and update the scraped and cleaned data to `outputdata/wikidata.csv`. Each row in the CSV file corresponds to a single Wikipedia page, with columns for the page title, page contents, and corresponding DBPedia information. The RDF graph generated by `testRDF.py` is not persisted to a file, but you can easily modify the script to do so if necessary.

## Customization

There are a few variables in `getData.py` that you can customize:

* `num_requests`: The number of Wikipedia pages to fetch. The default is 10,000.
* `wait_time`: The time to wait (in seconds) between HTTP requests to avoid overloading the server. The default is 0.5 seconds.
* `wait_period`: The number of iterations after which the script will wait. The default is 20.

Please ensure that your usage of these scripts complies with the Wikipedia and DBPedia terms of service, particularly with respect to the rate of requests. You should avoid making too many requests in a short period of time, as this could be disruptive to the services.

## Disclaimer

The scripts are provided as is, without any guarantees. The author is not responsible for any consequences of their use.