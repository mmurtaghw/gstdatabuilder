{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80f8586e-77a6-4fb6-ba3d-caf8cebd1da9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]ERROR:root:Error occurred: 'DataFrame' object has no attribute 'append'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okuōikojō Station\n",
      "Empty DataFrame\n",
      "Columns: [Title, PageContents, dbpediaTriples]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/10000 [00:06<19:21:25,  6.97s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 140\u001b[0m\n\u001b[1;32m    136\u001b[0m             time\u001b[38;5;241m.\u001b[39msleep(wait_time)  \u001b[38;5;66;03m# Wait for a defined number of seconds between each request.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 140\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 114\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    111\u001b[0m     infobox_content \u001b[38;5;241m=\u001b[39m extract_infobox_content(infobox)\n\u001b[1;32m    112\u001b[0m     page_contents \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m clean_text(get_infobox_content(infobox_content))\n\u001b[0;32m--> 114\u001b[0m dbpedia_triples \u001b[38;5;241m=\u001b[39m \u001b[43mget_dbpedia_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage_title\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m#dbpedia_triples = clean_text(get_dbpedia_info(page_title))\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mprint\u001b[39m(page_title)\n",
      "Cell \u001b[0;32mIn[6], line 67\u001b[0m, in \u001b[0;36mget_dbpedia_info\u001b[0;34m(title)\u001b[0m\n\u001b[1;32m     64\u001b[0m sparql\u001b[38;5;241m.\u001b[39msetQuery(query)\n\u001b[1;32m     65\u001b[0m sparql\u001b[38;5;241m.\u001b[39msetReturnFormat(JSON)\n\u001b[0;32m---> 67\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43msparql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m subject \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<http://dbpedia.org/resource/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitle\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m dbpedia_results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/SPARQLWrapper/Wrapper.py:1196\u001b[0m, in \u001b[0;36mQueryResult.convert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m _content_type_in_list(ct, _SPARQL_JSON):\n\u001b[1;32m   1195\u001b[0m     _validate_format(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJSON\u001b[39m\u001b[38;5;124m\"\u001b[39m, [JSON], ct, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequestedFormat)\n\u001b[0;32m-> 1196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convertJSON\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m _content_type_in_list(ct, _RDF_XML):\n\u001b[1;32m   1198\u001b[0m     _validate_format(\n\u001b[1;32m   1199\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDF/XML\u001b[39m\u001b[38;5;124m\"\u001b[39m, [RDF, XML, RDFXML], ct, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequestedFormat\n\u001b[1;32m   1200\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/SPARQLWrapper/Wrapper.py:1059\u001b[0m, in \u001b[0;36mQueryResult._convertJSON\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convertJSON\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[Any, Any]:\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;124;03m    Convert a JSON result into a Python dict. This method can be overwritten in a subclass\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;124;03m    for a different conversion method.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;124;03m    :rtype: dict\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1059\u001b[0m     json_str \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1060\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(json_str, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m   1061\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m json_str\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/http/client.py:482\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 482\u001b[0m         s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_safe_read\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m IncompleteRead:\n\u001b[1;32m    484\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/http/client.py:631\u001b[0m, in \u001b[0;36mHTTPResponse._safe_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_safe_read\u001b[39m(\u001b[38;5;28mself\u001b[39m, amt):\n\u001b[1;32m    625\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Read the number of bytes requested.\u001b[39;00m\n\u001b[1;32m    626\u001b[0m \n\u001b[1;32m    627\u001b[0m \u001b[38;5;124;03m    This function should be used when <amt> bytes \"should\" be present for\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;124;03m    reading. If the bytes are truly not available (due to EOF), then the\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;124;03m    IncompleteRead exception can be used to detect the problem.\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m<\u001b[39m amt:\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(data, amt\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(data))\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1275\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1276\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1277\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import csv\n",
    "import time\n",
    "import logging\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text of citation links\"\"\"\n",
    "    return re.sub(r'\\[\\d+\\]', '', text)\n",
    "\n",
    "def get_paragraphs(soup):\n",
    "    \"\"\"Find and return paragraph 'p' tags from the soup object\"\"\"\n",
    "    paragraphs = soup.find_all('p')\n",
    "    return ' '.join([clean_text(p.get_text()) for p in paragraphs])\n",
    "\n",
    "def get_tables(soup):\n",
    "    \"\"\"Find all tables on the page, extract content, and return it as a string\"\"\"\n",
    "    tables = soup.find_all('table')\n",
    "    table_text = ''\n",
    "    for table in tables:\n",
    "        rows = table.find_all('tr')\n",
    "        for row in rows:\n",
    "            columns = row.find_all('td')\n",
    "            if columns:\n",
    "                table_text += ' '.join([clean_text(col.get_text()) for col in columns])\n",
    "    return table_text\n",
    "\n",
    "def get_infobox_content(infobox_content):\n",
    "    \"\"\"Return the content of the infobox as a string\"\"\"\n",
    "    return ' '.join([f\"{clean_text(k)}: {clean_text(v)}\" for k, v in infobox_content.items()])\n",
    "\n",
    "def extract_infobox_content(infobox):\n",
    "    \"\"\"Extract rows from infobox and return a dictionary with their content\"\"\"\n",
    "    rows = infobox.find_all('tr')\n",
    "    infobox_content = {}\n",
    "    for row in rows:\n",
    "        header = row.find('th')\n",
    "        data = row.find('td')\n",
    "        if header and data:\n",
    "            infobox_content[header.text.strip()] = data.text.strip()\n",
    "    return infobox_content\n",
    "\n",
    "def clean_html_tags(text):\n",
    "    \"\"\"Clean text of HTML tags\"\"\"\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    return soup.get_text()\n",
    "\n",
    "def get_dbpedia_info(title):\n",
    "    \"\"\"Get and return information from DBpedia as a string\"\"\"\n",
    "    # remove any HTML tags from the title\n",
    "    title = clean_html_tags(title)\n",
    "    query = \"\"\"\n",
    "    SELECT ?predicate ?object WHERE {{\n",
    "      <http://dbpedia.org/resource/{}> ?predicate ?object\n",
    "    }}\n",
    "    \"\"\".format(title.replace(\" \", \"_\"))\n",
    "\n",
    "    sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "\n",
    "    results = sparql.query().convert()\n",
    "\n",
    "    subject = f\"<http://dbpedia.org/resource/{title.replace(' ', '_')}>\"\n",
    "    dbpedia_results = []\n",
    "    for result in results[\"results\"][\"bindings\"]:\n",
    "        predicate = f\"<{result['predicate']['value']}>\"\n",
    "        if result['object']['type'] == 'uri':\n",
    "            obj = f\"<{result['object']['value']}>\"\n",
    "        else:\n",
    "            obj = f\"\\\"{result['object']['value']}\\\"\"\n",
    "        dbpedia_results.append(f\"{subject} {predicate} {obj} .\")\n",
    "    return '\\n'.join(dbpedia_results)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the script\"\"\"\n",
    "    num_requests = 10000  # Number of requests to be made. Modify this as per your requirement.\n",
    "    wait_time = 0.5  # Wait time in seconds between requests.\n",
    "    wait_period = 20\n",
    "\n",
    "    csv_filename = 'outputData/wikidata.csv'\n",
    "\n",
    "    # Check if CSV file exists, if so initialize DataFrame with the same structure,\n",
    "    # else initialize a new one\n",
    "    if os.path.isfile(csv_filename):\n",
    "        existing_df = pd.read_csv(csv_filename, nrows=0)\n",
    "        data = pd.DataFrame(columns=existing_df.columns)\n",
    "    else:\n",
    "        data = pd.DataFrame(columns=[\"Title\", \"PageContents\", \"dbpediaTriples\"])\n",
    "\n",
    "    for i in tqdm(range(num_requests)):\n",
    "\n",
    "        url = \"https://en.wikipedia.org/api/rest_v1/page/random/html\"\n",
    "        try:\n",
    "            response_html = requests.get(url)\n",
    "            response_html.raise_for_status()  # raise exception for status codes like 4xx and 5xx\n",
    "            soup = BeautifulSoup(response_html.content, 'html.parser')\n",
    "            page_title = clean_text(soup.title.string.replace(\" - Wikipedia\", \"\").strip())\n",
    "            page_title = clean_html_tags(page_title) \n",
    "\n",
    "            page_contents = clean_text(get_paragraphs(soup))\n",
    "            page_contents += ' ' + clean_text(get_tables(soup))\n",
    "            infobox = soup.find('table', {'class': 'infobox'})\n",
    "            if infobox is not None:\n",
    "                infobox_content = extract_infobox_content(infobox)\n",
    "                page_contents += ' ' + clean_text(get_infobox_content(infobox_content))\n",
    "\n",
    "            dbpedia_triples = get_dbpedia_info(page_title)\n",
    "            #dbpedia_triples = clean_text(get_dbpedia_info(page_title))\n",
    "            \n",
    "            print(page_title)\n",
    "            print(data)\n",
    "            # Append the row to the DataFrame\n",
    "            new_row = pd.DataFrame({\"Title\": [page_title], \n",
    "                                    \"PageContents\": [page_contents], \n",
    "                                    \"dbpediaTriples\": [dbpedia_triples]})\n",
    "\n",
    "            data = pd.concat([data, new_row], ignore_index=True)\n",
    "\n",
    "            # If csv file does not exist, write with header, else append without writing header\n",
    "            if i == 0 and not os.path.isfile(csv_filename):\n",
    "                data.to_csv(csv_filename, mode='a', index=False)\n",
    "            else:\n",
    "                data.to_csv(csv_filename, mode='a', index=False, header=False)\n",
    "\n",
    "            # Clear data to prevent high memory usage\n",
    "            data = data.iloc[0:0]\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error occurred: {str(e)}\")\n",
    "\n",
    "        if i % wait_period == 0: \n",
    "            time.sleep(wait_time)  # Wait for a defined number of seconds between each request.\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "719a23e1-bffd-45e0-ad30-c2e4754f5bec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f169df22-aceb-4be8-8b37-db1fed9501c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
